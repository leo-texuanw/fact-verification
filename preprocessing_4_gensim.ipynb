{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Gensim dictionary and corpus, then store to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time\n",
    "from os import listdir\n",
    "\n",
    "CORPUS_DIR = './processed_wiki/'\n",
    "OBJECT_DIR = './objects/'\n",
    "SIMILARITY_CORPUS = './similarity_corpus'\n",
    "\n",
    "processed_corpus = listdir(CORPUS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "def load_corpus(corpus_file, docs, path=CORPUS_DIR):\n",
    "    partial_docs = {}\n",
    "    with open(join(path, corpus_file), 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            title = line[0]\n",
    "            partial_docs[title] = partial_docs.get(title, []) + line[2:]\n",
    "    docs.update(partial_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5396106 docs in all.\n",
      "takes 134.1604151725769 seconds\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Process, Manager\n",
    "from functools import partial\n",
    "\n",
    "start = time()\n",
    "\n",
    "pool = Pool(processes = 12)\n",
    "\n",
    "manager = Manager()\n",
    "docs = manager.dict()\n",
    "\n",
    "pool.map(partial(load_corpus, docs=docs), processed_corpus)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(len(docs), \"docs in all.\")\n",
    "print(\"takes\", time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5396106/5396106 [08:40<00:00, 10359.63it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "contents = [] # {doc_id, title}\n",
    "titles = {}\n",
    "i = 0\n",
    "\n",
    "for title in tqdm(docs.keys()):\n",
    "    titles[i] = title\n",
    "    i += 1\n",
    "    contents.append(docs[title])\n",
    "\n",
    "del docs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dictionary and store to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final word dictionary size: 558522\n",
      "This step takes 25.36000674565633 mins\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "start = time()\n",
    "\n",
    "word_dict = Dictionary(contents)\n",
    "# Filter out tokens in dict by frequency:\n",
    "# no_below: Keep tokens which are contained in at least no_below documents\n",
    "# no_above: Keep tokens which are contained in no more than no_above documents\n",
    "#   (fraction of total corpus size, not an absolute number).\n",
    "word_dict.filter_extremes(no_below=5, no_above=0.5, keep_n=700000) # 2002971 -> 558522\n",
    "print(\"final word dictionary size:\", len(word_dict))\n",
    "\n",
    "# store the dictionary\n",
    "# load by: corpora.Dictionary.load(path)\n",
    "word_dict.save(join(OBJECT_DIR, 'wiki_gensim_70.dict'))\n",
    "\n",
    "print(\"This step takes\", (time() - start) / 60, \"mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save represented corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This step takes 8.97870710293452 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "contents = [word_dict.doc2bow(doc) for doc in contents]  # convert corpus to BoW format\n",
    "\n",
    "# store to disk\n",
    "# load by: corpora.MmCorpus(path)\n",
    "corpora.MmCorpus.serialize(join(SIMILARITY_CORPUS, 'wiki_corpus_gensim_70.mm'), contents)\n",
    "\n",
    "print(\"This step takes\", (time() - start) / 60, \"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(join(OBJECT_DIR, 'titles_gensim_70.pkl'), 'wb') as f_docs:\n",
    "        pickle.dump(titles, f_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
