{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "DIR = './objects'\n",
    "TITLES = 'titles_gensim_70.pkl'\n",
    "TRAIN_SET = './train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = load_archive(\n",
    "    \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")\n",
    "\n",
    "predictor = Predictor.from_archive(archive, 'constituency-parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "result = predictor.predict_json({\"sentence\": sentence})\n",
    "\n",
    "print(result.keys())\n",
    "pos_tags = result['pos_tags']\n",
    "print(pos_tags)\n",
    "tokens = result['tokens']\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['hierplane_tree']['root'])\n",
    "# keys of result['hierplane_tree']['root']: ['word', 'nodeType', 'attributes', 'link', 'children']\n",
    "for child in result['hierplane_tree']['root']['children']:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc doc retrieval accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(pickle.load(open(join(DIR, TITLES), \"rb\" )).values())\n",
    "\n",
    "def title_without_parentheses(title):\n",
    "    return re.sub('-LRB-.*-RRB-', '', title).strip('_')\n",
    "\n",
    "without_parentheses = []\n",
    "for title in titles:\n",
    "    tit = title_without_parentheses(title)\n",
    "    if title != tit:\n",
    "        without_parentheses.append(tit)\n",
    "\n",
    "titles += without_parentheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_SET, 'r') as train_set_f:\n",
    "    train_set = json.load(train_set_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = ['-LRB-', '-RRB-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NPs(parse_result):\n",
    "    # TODO: \n",
    "    # 1. everything before verb as a NP\n",
    "    # 2. deal with different encoding\n",
    "\n",
    "    NPs, NP = set(), []\n",
    "    \n",
    "    # by hierplane_tree\n",
    "    hierplane_tree_children = parse_result['hierplane_tree']['root']['children']\n",
    "    for child in hierplane_tree_children:\n",
    "        if child['nodeType'] in ['NP', 'HYPH'] :\n",
    "            NP += child['word'].split()\n",
    "        elif NP:\n",
    "            NPs.add(\"_\".join(NP))\n",
    "            NP = []\n",
    "            \n",
    "    if NP:\n",
    "        NPs.append(NP)\n",
    "        NP = []\n",
    "        \n",
    "    # by customised rules\n",
    "    pos_tags = parse_result['pos_tags']\n",
    "    tokens = parse_result['tokens']\n",
    "    \n",
    "    for id_, tag in enumerate(pos_tags):\n",
    "        if tag in ['NP', 'NNP', 'HYPH']:\n",
    "            NP.append(tokens[id_])\n",
    "        elif tag in ignore_list:\n",
    "            NP.append(tag)\n",
    "        elif NP:\n",
    "            NPs.add(\"_\".join(NP))\n",
    "            NP = []\n",
    "\n",
    "    return list(map(lambda NP: re.sub('_-_', '-', NP), NPs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_evidence, total_evidence, true_evidence = 0, 0, 0\n",
    "\n",
    "for _id, record in tqdm(train_set.items()):\n",
    "    evidence = list(map(lambda x: x[0], record['evidence']))\n",
    "    true_evidence += len(evidence)\n",
    "    \n",
    "    parse_result = predictor.predict_json({\"sentence\": record['claim']})\n",
    "    NPs = get_NPs(parse_result)\n",
    "    total_evidence += len(NPs)\n",
    "            \n",
    "    missing = []\n",
    "    for evi in evidence:\n",
    "        got = False\n",
    "        for NP in NPs:\n",
    "            if NP in evi:\n",
    "                got = True\n",
    "                break\n",
    "        if got:\n",
    "            found_evidence += 1\n",
    "        else:\n",
    "            missing.append(evi)\n",
    "\n",
    "    if missing:\n",
    "        print('In claim:', record['claim'])\n",
    "        print('    NPs:', NPs)\n",
    "        print('    missing:', missing)\n",
    "        #print('    POS:', (pred_result['hierplane_tree']['root']['children']))\n",
    "        print('    POS:', parse_result['pos_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_evidence, found_evidence, total_evidence)\n",
    "print(\"accuracy:\", found_evidence / true_evidence)\n",
    "print(\"recall:\", found_evidence / total_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
